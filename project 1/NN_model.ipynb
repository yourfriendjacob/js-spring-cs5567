{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Tncw9ZWXySU30cDktBdyymb87WTIVCvC","timestamp":1709242238703}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Neural Network Sample"],"metadata":{"id":"IWPgCHtXwz8V"}},{"cell_type":"markdown","source":["## Building a neural network"],"metadata":{"id":"rxbLpLVWyMSQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y74U8VXyf_yg"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from statistics import mean\n","from typing import Dict, List, Tuple\n","\n","np.random.seed(7191)\n","\n","class Neural:\n","\n","    def __init__(self, layers: List[int], epochs: int,\n","                 learning_rate: float = 0.001, batch_size: int=32,\n","                 validation_split: float = 0.2, verbose: int=1):\n","        self._layer_structure: List[int] = layers\n","        self._batch_size: int = batch_size\n","        self._epochs: int = epochs\n","        self._learning_rate: float = learning_rate\n","        self._validation_split: float = validation_split\n","        self._verbose: int = verbose\n","        self._losses: Dict[str, float] = {\"train\": [], \"validation\": []}\n","        self._is_fit: bool = False\n","        self.__layers = None\n","\n","    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n","        # validation split\n","        X, X_val, y, y_val = train_test_split(X, y, test_size=self._validation_split, random_state=42)\n","        # initialization of layers\n","        self.__layers = self.__init_layers()\n","        for epoch in range(self._epochs):\n","            epoch_losses = []\n","            for i in range(1, len(self.__layers)):\n","                # forward pass\n","                x_batch = X[i:(i+self._batch_size)]\n","                y_batch = y[i:(i+self._batch_size)]\n","                pred, hidden = self.__forward(x_batch)\n","                # calculate loss\n","                loss = self.__calculate_loss(y_batch, pred)\n","                epoch_losses.append(np.mean(loss ** 2))\n","                #backward\n","                self.__backward(hidden, loss)\n","            valid_preds, _ = self.__forward(X_val)\n","            train_loss = mean(epoch_losses)\n","            valid_loss = np.mean(self.__calculate_mse(valid_preds,y_val))\n","            self._losses[\"train\"].append(train_loss)\n","            self._losses[\"validation\"].append(valid_loss)\n","            if self._verbose:\n","                print(f\"Epoch: {epoch} Train MSE: {train_loss} Valid MSE: {valid_loss}\")\n","        self._is_fit = True\n","        return\n","\n","    def predict(self, X: np.ndarray) -> np.ndarray:\n","        if self._is_fit == False:\n","            raise Exception(\"Model has not been trained yet.\")\n","        pred, hidden = self.__forward(X)\n","        return pred\n","\n","    def plot_learning(self) -> None:\n","        plt.plot(self._losses[\"train\"],label=\"loss\")\n","        plt.plot(self._losses[\"validation\"],label=\"validation\")\n","        plt.legend()\n","\n","    def __init_layers(self) -> List[np.ndarray]:\n","        layers = []\n","        for i in range(1, len(self._layer_structure)):\n","            layers.append([\n","                np.random.rand(self._layer_structure[i-1], self._layer_structure[i]) / 5 - .1,\n","                np.ones((1,self._layer_structure[i]))\n","            ])\n","        return layers\n","\n","    def __forward(self, batch: np.ndarray) -> Tuple[np.ndarray, List[np.ndarray]]:\n","        hidden = [batch.copy()]\n","        for i in range(len(self.__layers)):\n","            batch = np.matmul(batch, self.__layers[i][0]) + self.__layers[i][1]\n","            if i < len(self.__layers) - 1:\n","                batch = np.maximum(batch, 0)\n","            # Store the forward pass hidden values for use in backprop\n","            hidden.append(batch.copy())\n","        return batch, hidden\n","\n","    def __calculate_loss(self,actual: np.ndarray, predicted: np.ndarray) -> np.ndarray:\n","        \"mse\"\n","        return predicted - actual\n","\n","\n","    def __calculate_mse(self, actual: np.ndarray, predicted: np.ndarray) -> np.ndarray:\n","        return (actual - predicted) ** 2\n","\n","    def __backward(self, hidden: List[np.ndarray], grad: np.ndarray) -> None:\n","        for i in range(len(self.__layers)-1, -1, -1):\n","            if i != len(self.__layers) - 1:\n","                grad = np.multiply(grad, np.heaviside(hidden[i+1], 0))\n","\n","            w_grad = hidden[i].T @ grad\n","            b_grad = np.mean(grad, axis=0)\n","\n","            self.__layers[i][0] -= w_grad * self._learning_rate\n","            self.__layers[i][1] -= b_grad * self._learning_rate\n","\n","            grad = grad @ self.__layers[i][0].T\n","        return\n","#Letâ€™s generate some dummy data to test the Neural.\n","\n","def generate_data():\n","    # Define correlation values\n","    corr_a = 0.8\n","    corr_b = 0.4\n","    corr_c = -0.2\n","\n","    # Generate independent features\n","    a = np.random.normal(0, 1, size=100000)\n","    b = np.random.normal(0, 1, size=100000)\n","    c = np.random.normal(0, 1, size=100000)\n","    d = np.random.randint(0, 4, size=100000)\n","    e = np.random.binomial(1, 0.5, size=100000)\n","\n","    # Generate target feature based on independent features\n","    target = 50 + corr_a*a + corr_b*b + corr_c*c + d*10 + 20*e + np.random.normal(0, 10, size=100000)\n","\n","    # Create DataFrame with all features\n","    df = pd.DataFrame({'a': a, 'b': b, 'c': c, 'd': d, 'e': e, 'target': target})\n","    return df"]},{"cell_type":"markdown","source":["## Fitting the model on randomly generated data"],"metadata":{"id":"4oketVXv1MgV"}},{"cell_type":"code","source":["df = generate_data()\n","\n","# Separate the features and target\n","X = df.drop('target', axis=1)\n","y = df['target']\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","y_train = y_train.to_numpy().reshape(-1,1)\n","y_test = y_test.to_numpy().reshape(-1,1)\n","\n","layer_structure = [X_train.shape[1],10,10,1]\n","nn = Neural(layer_structure, 20, 1e-5, 64, 0.2, 1)\n","\n","nn.fit(X_train, y_train)\n","\n","y_pred = nn.predict(X_test)\n","nn.plot_learning()\n","\n","print(\"Test error: \",mean_squared_error(y_test, y_pred))"],"metadata":{"id":"fk6YL6G4gXQj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### randomly generated data from above"],"metadata":{"id":"AKHoUCHv04K8"}},{"cell_type":"code","source":["df"],"metadata":{"id":"ULxAnACqZ0pO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Importing user/manual dataset and preprocessing according to the model"],"metadata":{"id":"QCiKJDHWwyZp"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Assume 'df' is your DataFrame with categorical variables\n","# Let's say 'categorical_columns' is a list of columns containing categorical data\n","\n","label_encoder = LabelEncoder()\n","\n","\n","# Load your dataset\n","df = pd.read_csv('data.csv')\n"],"metadata":{"id":"rSPZ8SJkkhJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pre-processed data, ready to pass for model fitting."],"metadata":{"id":"TIyoVI6j2PW8"}},{"cell_type":"code","source":["df"],"metadata":{"id":"5pOQuRiuvVjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["le = LabelEncoder()\n","df['diagnosis'] = le.fit_transform(df['diagnosis'])\n","\n","df = df.drop('id',axis=1)\n","df =  df.drop('Unnamed: 32',axis=1)"],"metadata":{"id":"XqSsJFC9a0Pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"RXJVtS6K8dEI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fitting the model on the imported pre-processed dataset"],"metadata":{"id":"sFyLkJfk3FhJ"}},{"cell_type":"markdown","source":["### Feature extraction using correlation analysis against target variable"],"metadata":{"id":"zIW6M5Cx8sRT"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","\n","# Calculate the correlation matrix\n","correlation_matrix = df.corr()\n","plt.figure(figsize=(30, 12))\n","sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Heatmap')\n","plt.show()\n"],"metadata":{"id":"mT74k_YrccGT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Feature extraction for model fitting"],"metadata":{"id":"jHMzhsrz9Ch-"}},{"cell_type":"code","source":["# Separate the features and target\n","X = df[['radius_mean','perimeter_mean','area_mean','concave points_worst','perimeter_worst','radius_worst','concave points_mean']]\n","y = df['diagnosis']"],"metadata":{"id":"qaUTsuvjvQ7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"d2HCgnPd23c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y"],"metadata":{"id":"PFpSS0fIatOL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fitting the Neural network model on the extracted features against the target variable and fine tuning the hyperparameters that will be used to configure the model."],"metadata":{"id":"nl_7BdKb9SBl"}},{"cell_type":"code","source":["scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","y_train = y_train.to_numpy().reshape(-1,1)\n","y_test = y_test.to_numpy().reshape(-1,1)\n","\n","epocs = 100\n","learn_rate = .0003\n","batch_size = 20\n","val_split = .2\n","verbose = 1\n","\n","layer_structure = [X_train.shape[1],2,2,1]\n","nn = Neural(layer_structure, epocs, learn_rate, batch_size, val_split, verbose)\n","\n","nn.fit(X_train, y_train)\n","\n","y_pred = nn.predict(X_test)\n","nn.plot_learning()\n","\n","print(\"Test error: \",mean_squared_error(y_test, y_pred))"],"metadata":{"id":"Hii3OJ0IasgD"},"execution_count":null,"outputs":[]}]}